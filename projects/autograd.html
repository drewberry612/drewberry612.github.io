<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Autograd Implementation from Scratch</title>
    <link rel="stylesheet" href="../styles/autograd.css">
</head>
<body>
    <header>
        <h1>Autograd with NumPy</h1>
        <div class="cv-link-container">
            <a href="https://github.com/drewberry612/autograd" target="_blank" class="cv-link">View on GitHub</a>
        </div>
    </header>
    <main>
        <section>
            <p>This small project involves implementing autograd functionality from scratch using NumPy to train a neural network. The goal was to build the backpropagation mechanism without relying on libraries like PyTorch or TensorFlow, providing a deeper understanding of the inner workings of neural network training.</p>
        </section>
        <h2>Project Details</h2>
        <section>
            <p><strong>Objective:</strong> Implement autograd for neural network training using only NumPy.</p>
            <p><strong>Key Focus:</strong></p>
            <ul>
                <li>Backpropagation</li>
                <li>Automatic differentiation</li>
                <li>Weight updates using gradient descent</li>
            </ul>
            <p>By building this from scratch, the project provides valuable insight into how deep learning frameworks manage gradients and optimize model parameters.</p>
        </section>
        <h2>Neural Network</h2>
        <section>
            <p>This project covers the implementation of a basic feedforward neural network that uses:</p>
            <ul>
                <li>Sigmoid activation function</li>
                <li>Mean Squared Error loss function</li>
            </ul>
            <p>The network is trained using gradient descent with the gradients calculated manually.</p>
        </section>
    </main>
    <footer>
        <p>Â© 2025 Drew Berry. References available upon request.</p>
    </footer>
</body>
</html>
