<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Drew Berry | MSc Thesis</title>
    <link rel="stylesheet" href="../styles/thesis.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/slick-carousel/slick/slick.css"/>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/slick-carousel/slick/slick.min.js"></script>
    <script src="../scripts/video-carousel.js"></script>
</head>
<body>
    <header>
        <h1>Reinforcement Learning in TORCS</h1>
        <div class="cv-link-container">
            <a href="https://github.com/drewberry612/torcs-research" target="_blank" class="cv-link">View on GitHub</a>
            <a href="../pdfs/Thesis.pdf" target="_blank" class="cv-link">View Thesis</a>
        </div>
    </header>
    <main>
        <section>
            <p>"Using Machine Learning Techniques to Create Neural Network Driving Agents, with a Focus on Reinforcement Learning"</p>
            <p>For my MSc thesis, I trained AI driving agents using a genetic algorithm and the PPO reinforcement learning algorithm. The TORCS (The Open Racing Car Simulator) was used as the environment for training these agents.</p>
        </section>
        <h2>Agent Demontrations</h2>
        <section>
            <div class="video-carousel-container">
                <div id="video-title" class="video-title">GA 1</div>
                <div class="video-carousel">
                    <div data-title="GA 1" data-description="This video showcases the best-performing agent produced during my thesis research—trained using a genetic algorithm with a simple distance-based fitness function. The agent consistently follows the racing line, maintaining a top speed of 140km/h while keeping a safe distance from track edges. Its driving behaviour balances speed and control, completing laps reliably and mimicking human-like racing patterns. Though it lacks braking capabilities, the agent's ability to optimise track position and maintain momentum makes it the most stable and effective driver model developed during the project.">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/83Cg7GbVDVY" 
                            title="GA 1" 
                            frameborder="0" 
                            allowfullscreen>
                        </iframe>
                    </div>
                    <div data-title="PPO 1" data-description="This video presents the most successful agent trained using Proximal Policy Optimisation (PPO) during my thesis. Although its behaviour remains slightly erratic—often adjusting steering abruptly—the agent does attempt to follow the racing line, revealing glimpses of more advanced racing strategies. Trained with a modified reward function designed to promote racing efficiency, this model reflects the potential of reinforcement learning in developing nuanced driving behaviours, even if the results remain less stable than those of the genetic algorithm. Its performance demonstrates both the promise and the challenges of fine-tuning PPO in complex environments like TORCS.">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/q4tM7_CCrZA" 
                            title="PPO 1" 
                            frameborder="0" 
                            allowfullscreen>
                        </iframe>
                    </div>
                    <div data-title="GA 2" data-description="This video features GA 2, a genetic algorithm agent trained using a modified fitness function that encourages adherence to the track axis rather than the racing line. As a result, the agent prioritises staying centred on the track over taking optimal corners, producing smoother but less aggressive driving behaviour. While this approach allows for consistent lap completion, it results in slightly slower lap times compared to GA 1. The agent maintains a steady top speed of 140km/h and demonstrates reliable performance, making it a strong example of how simple fitness functions can shape distinct driving strategies.">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/jIdxrvjkvSk" 
                            title="GA 2" 
                            frameborder="0" 
                            allowfullscreen>
                        </iframe>
                    </div>
                    <div data-title="PPO 2" data-description="This video shows PPO 2, a reinforcement learning agent trained using Proximal Policy Optimisation with the original reward function. Like PPO 1, the agent exhibits underdeveloped and occasionally erratic behaviour due to the stochastic nature of early training. However, this model learns to stick closely to the track axis, resulting in more conservative but stable driving. While it lacks the nuanced cornering seen in racing line strategies, PPO 2 reliably remains on the track at speeds up to 140km/h. Its performance highlights the importance of reward design in shaping emergent behaviours within reinforcement learning systems.">
                        <iframe width="560" height="315" src="https://www.youtube.com/embed/_cP2MIGxyMg" 
                            title="PPO 2" 
                            frameborder="0" 
                            allowfullscreen>
                        </iframe>
                    </div>
                </div>
                <div id="video-description" class="video-description">This video showcases the best-performing agent produced during my thesis research—trained using a genetic algorithm with a simple distance-based fitness function. The agent consistently follows the racing line, maintaining a top speed of 140km/h while keeping a safe distance from track edges. Its driving behaviour balances speed and control, completing laps reliably and mimicking human-like racing patterns. Though it lacks braking capabilities, the agent's ability to optimise track position and maintain momentum makes it the most stable and effective driver model developed during the project.</div>
            </div>
        </section>
        <h2>Project Overview</h2>
        <section>
            <p>The Open Racing Car Simulator (TORCS) is a racing simulation game where players race against computer-controlled opponents. In this project, I developed several AI agents for the game, using various machine learning techniques on neural networks. These agents were trained to complete laps efficiently and compete with human players.</p>
            <p>The focus was on genetic algorithms and reinforcement learning (PPO), which were tested and compared to determine their effectiveness in training the AI agents.</p>
        </section>
        <h2>Machine Learning Approach</h2>
        <section>
            <p><strong>Genetic Algorithm (GA):</strong> A form of evolutionary algorithm that uses a population of agents and evolves them through selection, crossover, and mutation. This method was experimented with in-depth to create agents that could successfully navigate the racing environment.</p>
            <p><strong>PPO (Proximal Policy Optimization):</strong> A reinforcement learning algorithm that was used to train agents to make decisions based on rewards and penalties in the racing environment.</p>
            <p>These algorithms were applied to a set of agents, each trained with different configurations, to explore the advantages and challenges of each technique in a racing simulation.</p>
        </section>
        <h2>Racing Simulation Environment</h2>
        <section>
            <p>The TORCS environment offers challenges such as high-speed decision-making and spatial awareness, which closely mimic those faced by real-world autonomous vehicles. The research aimed to uncover insights into the application of machine learning in autonomous driving.</p>
            <p>Each algorithm was carefully tuned to create agents that could follow the track axis and racing lines, simulating behaviors that would be useful in real-world autonomous driving scenarios.</p>
        </section>
        <h2>Abstract</h2>
        <section>
            <p>The Open Racing Car Simulator (TORCS) is a car racing simulation game, which allows a player to race against opponents that are simulated by the computer. In this project, several AI opponents for this game have been developed, using various forms of machine learning on neural networks, which can be observed or raced against. The differences and advantages of genetic and reinforcement learning algorithms have been explored in this racing game context. Each parameter and condition of these algorithms were experimented upon diligently, to create AI agents that could complete laps of a race efficiently and rival human players.</p>
            <p>A racing simulation environment offers a unique set of challenges that are comparable to those faced by real-world autonomous vehicles, such as high-speed decision-making and spatial awareness. The exploration of this idea could uncover insights into the use of machine learning in such contexts.</p>
            <p>This thesis illustrates all experiments and findings, demonstrating the competency of trained agents, while also explaining the technical aspects of how research was conducted. Both algorithms have created simple and complex racing behaviours, such as following the track axis and/or racing line. However, a final model trained using the genetic algorithm demonstrated the best performance.</p>
        </section>
        <section class="skills-stack-container">
            <div class="skills-box-container">
                <h2>Skills</h2>
                <div class="skills-box">
                    <ul>
                        <li>Reinforcement Learning</li>
                        <li>Genetic Algorithms</li>
                        <li>Research & Experimentation</li>
                        <li>Machine Learning</li>
                        <li>PPO (Proximal Policy Optimization)</li>
                        <li>Neural Network Training</li>
                        <li>Racing Simulation Environments</li>
                        <li>Hyperparameter Tuning</li>
                        <li>Algorithm Comparison</li>
                        <li>Autonomous Driving Simulation</li>
                        <li>Data Visualisation</li>
                    </ul>
                </div>
            </div>
            <div class="stack-box-container">
                <h2>Tech Stack</h2>
                <div class="stack-box">
                    <ul>
                        <li>Python</li>
                        <li>TORCS</li>
                        <li>OpenAI Gym</li>
                        <li>Git</li>
                        <li>StableBaselines3</li>
                        <li>NumPy</li>
                        <li>tkinter</li>
                    </ul>
                </div>
            </div>
        </section>
    </main>
    <footer>
        <p>© 2025 Drew Berry. References available upon request.</p>
    </footer>
</body>
</html>
